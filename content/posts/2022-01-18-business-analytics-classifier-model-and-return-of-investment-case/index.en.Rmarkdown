---
title: Business Analytics | Classifier Model and Return of Investment Case
author: Giuliano Sposito
date: '2022-01-18'
slug: 'classifier-model-and-return-of-investment'
categories:
  - data science
  - advanced business analytics
tags:
  - evaluation
  - machine learning
  - model
  - data science
  - classifier
  - random forest
  - metrics
subtitle: ''
lastmod: '2022-01-18T16:17:51-03:00'
authorLink: ''
description: ''
hiddenFromHomePage: no
hiddenFromSearch: no
featuredImage: 'images/post_cover.png'
featuredImagePreview: 'images/post_cover.png'
toc:
  enable: yes
math:
  enable: no
lightgallery: no
license: ''
---

```{r setup, echo = FALSE, warning = FALSE, message = FALSE}
# put rnotebook in the same workdir
# knitr::opts_knit$set(root.dir = normalizePath(rprojroot::find_rstudio_root_file())) 
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Can you imagine a real business case where you apply machine learning to build and Random Forest classifier and its accuracy of the model isn't the (only) main metric to pay attention? In real case scenarios the cost and benefits can affect a model in different aspects, this post exercises a business case where the return of an investment is dependent of the behavior of the precision metric.

<!--more-->

### Intro

Missed appointments cost the US healthcare system over $150 billion a year. Missed appointments directly cause loss of revenue and under-utilization of precious medical resources. It also leads to long patient waiting times and in the long run, leads to higher medical costs.

In this context lets build a predictive model to estimate if an patient will miss an appointment and use the prediction to take an action to try to avoid the cancellation.

### Dataset

We obtained a [data set](./assets/data.xlsx) with 7,463 medical appointments over a three year period at a specialized care clinic. In this data set, each row corresponds to an appointment and indicates whether it was cancelled or not. 

```{r loadData, cache=TRUE}
library(xlsx)
library(tidyverse)

# the life, the universe an everything else...
set.seed(42)

# data set
rawdata <- xlsx::read.xlsx("./assets/data.xlsx", sheetIndex = 1)

# basic clean_up
appdata <- rawdata %>% 
  as_tibble() %>% 
  janitor::clean_names() %>% 
  mutate_if(is.character, as.factor)

dim(appdata)
```

#### Data Overview

Lets see the overall aspect of the data set

```{r eda}
skimr::skim(appdata)
```

In total, 1662 out of 7463 appointments were cancelled, we can see it from column **status** (our target).

### Model

Since we are interested in appointment cancellations, the target variable is whether an appointment is cancelled or not and success in this particular context means that an appointment is cancelled.

```{r model}
library(tidymodels)

# training & test data partition
appsplit <- initial_split(appdata)

# basic transformation
apprecp <- appsplit %>% 
  training() %>% 
  recipe(status ~ ., data=.) %>% 
  update_role(date_id, new_role = "id variable") %>% 
  update_role(status, new_role = "outcome") %>% 
  step_log(lag) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  prep() 

# training & test set
app_tr <- juice(apprecp)
app_ts <- bake(apprecp, testing(appsplit))

# fit a model
app_model <- rand_forest(trees = 100, mode="classification") %>% 
  set_engine("ranger") %>% 
  fit(status ~ . - date_id, data=app_tr)
```

We build a simple and direct [random forest](https://en.wikipedia.org/wiki/Random_forest) model using [Ranger](https://www.rdocumentation.org/packages/ranger/versions/0.13.1/topics/ranger) implementation through [Tidymodels](https://www.tidymodels.org/) package. 

```{r seeTheModel}
app_model
```

#### Evaluation

How good is our prediction model?

```{r modelEval}
# eval it
app_pred <- predict(app_model, app_ts) %>%  # class outcome
  bind_cols(predict(app_model, app_ts, type = "prob")) %>% # class prob
  bind_cols(select(app_ts,status)) %>%  # true value
  relocate(status, everything())

# performance
cm <- app_pred %>% 
  conf_mat(status, .pred_class)

cm %>% 
  summary() %>% 
  select(-.estimator) %>% 
  knitr::kable()

# AUC
app_pred %T>% 
  roc_auc(.pred_Arrived, truth=status) %>% 
  roc_curve(.pred_Arrived, truth=status) %>% 
  autoplot()
```

### Taking a Business Decision to action

With the prediction in hands, we can make a business action to try revert appointments cancellations. Let's assume we can make a phone call to anyone predicted as "Cancelled" one or two days early and this will revert around 30% of the cancelled appointments. Is it a viable approach? Is it economically viable? At least, better than let the patient cancel his appointment?

To decide this we need to characterize the value of some variables, for this exercise we can assume:

* Cost of a phone call: $5
* Revert appointment cancellation rate: 30%
* Benefit of an appointment: $60

So, we can calculate the *Return of Investment* (_RoI_) of this action:

```{r calcRoI}

# business variable
phone_cost   <- 5
reverse_rate <- .3
benefit      <- 60

# business case

# cost: phone to all patient predicted as "canceled"
total_cost  <- sum(cm$table["Cancelled",]) * phone_cost

# benefit: the reverse of 30% of patient that (in fact) would cancel the appointment
total_benefit <- cm$table["Cancelled","Cancelled"] * reverse_rate * benefit

# return of the investment
RoI <- total_benefit - total_cost

```

So we come out with this result:

* Total Cost: `r sum(cm$table["Cancelled",]) ` * $`r phone_cost ` = $`r total_cost`
* Total Benefit: `r cm$table["Cancelled","Cancelled"]` * `r reverse_rate` * $`r benefit` = $`r total_benefit`
* Return: $`r total_benefit` - $`r total_cost` = $`r RoI`

As we saw, with a _RoI_ of $`r RoI` this action worth to be take.

One aspect essential to pay attention, we call to all patient predict as "cancellation" but we only get return over 30% (reversion rate) of those truly identified as "cancellation", a.k.a, **True Positives**. In other words, the cost of the action is function of **True Positives** plus **False Positives** and the benefit of the action is function only of **True Positives**. This is because a **False Positive** is a patient predicted as "cancellation" but heÂ´ll go to the appointment, so the phone call does not bring any benefit, only cost in those cases.

#### Improving Business Performance 

Can we do better return without improving the model? As the _RoI_ is function of the rate between **True Positives** and **True Positives** + **False Positives** (a.k.a _Precision Metric_) in proportion of the benefits and cost we can _tune_ our classifier changing the [_logistic threshold_](https://deepchecks.com/glossary/classification-threshold/) to change the _precision_ towards to try maximize the _RoI_. 

To see this in practice, calculating what happens with our _RoI_ changing the classifier threshold from 0 to 1 in increments of 0.01:

```{r thresholdRange}

# generate the confusion metrics in function of an threshold
genConfMatrix <- function(.threshold, .evalData){
  # reply as a tibble row
  # new truth vs prediction table
  tibble(
      truth=.evalData$status, 
      # the prediction as function of the customized threshold
      estimate=unique(.evalData$status)[as.integer(.evalData$.pred_Cancelled>=.threshold)+1]
    ) %>%
    # gen the confusion matrix
    conf_mat(truth, estimate) %>% 
    return()
}

# calculates the RoI based in the result of a confusion matrix
calcRoi <- function(.cm, .benefit=60, .cost=5, .rev_rate=.3){
  tibble(
    TP = .cm$table["Cancelled","Cancelled"],
    FP = .cm$table["Cancelled","Arrived"] ) %>% 
    mutate(
      # cost: phone to all patient predicted as "canceled"
      cost    = (TP+FP) * .cost,
      # benefit: the reverse of 30% of patient that (in fact) would cancel the appointment
      benefit = TP * .rev_rate * .benefit,
      # return of the investment
      roi = benefit-cost
    ) %>% 
    return()
}

# using threshold from 0 to 1 in 0.01 increments
simulations <- tibble(threshold = seq(0,1,.01)) %>% 
  mutate( 
    # gen the confusion matrix and roi values for this threshold
    cm  = map(threshold, genConfMatrix, .evalData=app_pred),
    roi = map(cm, calcRoi)
  ) %>% 
  unnest(roi)

# what we get
simulations %>% 
  select(-cm) %>% 
  head(10) %>% 
  knitr::kable()

# visualizing
simulations %>%           
  ggplot(aes(x=threshold, y=roi)) +
    # geom_point(size=2) +
    geom_line() +
    geom_hline(yintercept = 0, linetype="dashed", color="red") +
    geom_vline(xintercept = 0.5, linetype="dashed", color="darkgrey") +
    scale_x_continuous(breaks=seq(0,1,.1)) +
    ylim(c(-100,NA)) +
    labs(title="Return of Investment", subtitle = "Influence of the Threshold Parameter")
```

We can see the the _RoI_ is better using a threshold between 0.2 and 0.3, and not 0.5 as usually set, more than that, at this level we get worst _accuracy_ for the model, take a look:

```{r worstAcc}

simulations %>% 
  filter(threshold==.3) %>% 
  pull(cm) %>% 
  .[[1]] %>% 
  summary() %>% 
  select(-.estimator) %>% 
  knitr::kable()


```
 
```{r oldAcc, echo=FALSE}
newAcc <- simulations %>% 
  filter(threshold==.3) %>% 
  pull(cm) %>% 
  .[[1]] %>% 
  summary() %>% 
  filter(.metric=="accuracy") %>% 
  pull(.estimate)

oldAcc <- cm %>% 
  summary() %>% 
  filter(.metric=="accuracy") %>% 
  pull(.estimate)

```

Compare this the _accuracy_ metric (`r newAcc`) with the value obtained in the first calculation above (`r oldAcc`). 

### Conclusion and Classification Metrics

As we saw, a business decision is not only function of the accuracy of the model, costs and benefits can affect different aspects of the model. Remember to return to real life when applying the model in business case scenarios, to optimize the correct target.

And, be aware of the classification metrics.

![Classification Metrics](./images/classification_metrics.png)

```{r classMetricBehavior}
simulations %>% 
  mutate(
    metrics = map(cm, function(.x){
      .x %>% 
        summary() %>% 
        select(-.estimator) %>% 
        pivot_wider(names_from = .metric, values_from = .estimate) %>% 
        return()
    })) %>% 
  unnest(metrics) %>%
  select(threshold, accuracy, sens, spec, precision, recall) %>%
  pivot_longer(cols = -threshold, names_to = "metric", values_to = "value") %>% 
  ggplot(aes(x=threshold, y=value, color=metric))+
  geom_line() +
  labs(title="Main Classification Matrics", subtitle="Behavior in function of the logistic threshold") +
  theme_minimal()
```
