---
title: Power Analysis | Introduction
author: Giuliano Sposito
date: '2021-11-20'
slug: 'bit-by-Bit-power-analysis-01-02'
categories:
  - data science
tags:
  - rstats
  - power analysis
  - hypothesis test
  - simulation
  - sample size
subtitle: 'Power Analysis - Part 01 - Intro'
lastmod: '2021-11-15T17:38:47-03:00'
draft: yes
authorLink: ''
description: ''
hiddenFromHomePage: no
hiddenFromSearch: no
featuredImage: 'images/hypothesis.jpg'
featuredImagePreview: 'images/hypothesis.jpg'
toc:
  enable: yes
math:
  enable: yes
lightgallery: no
license: ''
---

This post explore Power Analysis technique. Power is the probability of detecting an effect, given that the effect is really there. In other words, it is the probability of rejecting the null hypothesis when it is in fact false. For example, let’s say that we have a simple study with drug A and a placebo group, and that the drug truly is effective; the power is the probability of finding a difference between the two groups.[^ucla]

<!--more-->

### Introduction

So, imagine that we had a power of .8 and that this simple study was conducted many times. Having **power of .8 means that 80% of the time**, we would **get a statistically significant difference** between the drug A and placebo groups. This also means that 20% of the times that we run this experiment, we will not obtain a statistically significant effect between the two groups, even though there really is an effect in reality.

Perhaps **the most common use is to determine the necessary number of subjects needed to detect an effect of a given size**. Note that trying to find the absolute, bare minimum number of subjects needed in the study is often not a good idea. Additionally, **power analysis can be used to determine power, given an effect size and the number of subjects available**. You might do this when you know, for example, that only 75 subjects are available (or that you only have the budget for 75 subjects), and you want to know if you will have enough power to justify actually doing the study. In most cases, **there is really no point to conducting a study that is seriously underpowered**. 

Besides the issue of the number of necessary subjects, there are other good reasons for doing a power analysis. For example, a power analysis is often required as part of a grant proposal.  And finally, doing a power analysis is often just part of doing good research; A power analysis is a good way of making sure that you have thought through every aspect of the study and the statistical analysis before you start collecting data.

```{r setup, echo=FALSE}
# default chunk behavior
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache=TRUE)
set.seed(1234) 
```

### Examples

#### Finding The Sample Size

We'll apply *power analysis* in its case more common, to determine the necessary number of subjects to detect a given effect, in this case let's use the [`{pwr package}`](https://cran.r-project.org/web/packages/pwr/pwr.pdf) in a scenario of drug treatment. Lets consider a control group and a treatment group to COVID-19, for example. To simplify the case we assume that the recovery time for COVID-19 is normally distributed around 21.91 days (mean) and standard deviation of 5.33 days[^covid], how many subjects we will have to had to detect a treatment that can shorter the recovery in 5 days?


```{r simpleCasePops}
# install.packages("pwr")
library(pwr)
library(broom)
library(tidyverse)

# covid recovery time (mean and sd)
mo <- 21.91
s0 <- 5.33

# we want to detect 5 day early-recovery time (at same standard deviation) 
mt <- mo-5

# simulation the populations
popCntrl <- rnorm(10000, mo, s0) #control
popTreat <- rnorm(10000, mt, s0) #under treatment

# lets see the populations 
data.frame(
  recoveryTime = c(popCntrl, popTreat),
  group = rep(c("control","popTreat"), each=10000)
) %>%
  ggplot(aes(x=recoveryTime, fill=group, group=group))+
  geom_histogram(alpha=.5, position = 'identity') +
  theme_minimal() +
  labs(title = "Population Distribuition", subtitle = "Comparing control group and treatment group")
```

Geramos duas populações em que a separação da média, ou seja, o tamanho do efeito do tratamento é bem evidente. Vamos fazer a análise de poder para tentar detectar uma separação como essa, de ~5 dias.

A fórmula de calculo do tamanho do efeito para ser usada neste caso é a distância entre médias em desvio padrões[^stats]:

$$ d=\frac{|\mu_{control}-\mu_{popTreat}|}{\sigma} $$

where $ \mu $ são as médias dos grupos e $ \sigma $ é o desvio padrão, que vamos considerar (para facilitar) o mesmo entre as populações. Assim podemos usar o pacote `{pwr}` para calcular o número de amostras das populações que nos garatam testar, com 80% de segurança, a nulabilidade de uma hipótese com 0.05 de significância.

```{r sampleSize}
# size efect 21.91 to 16.91
# in this case (simple t.test) the effect size is the mean difference in
# standard deviations (like z-score)
es <- (mo-mt)/s0

# Power Analysis 
pa <- pwr.2p.test(sig.level = 0.05, power = .8, h = es)
pa
```

Então, o tamanho mínimo das amostras para esse caso é `r ceiling(pa$n)` indicado pelo parâmetro `n`. Vamos testar:

```{r hypotesesTest}
smpC <- sample(popCntrl, ceiling(pa$n))
smpT <- sample(popTreat, ceiling(pa$n))

t <- t.test(smpT, smpC)
t
```

Podemos ver que conseguimos testar a hipotese com um `p.value` de `r t$p.value`, mostrando que as amostras podem ter vindo de fato de duas populações diferentes.

Para entender melhor o numero, vamos ver como o `p.value` deste caso se comporta diante de tamanho de amostras diferentes (algo como fazer o _p.hacking_):

```{r phaking}

# tamanhos de amostras para testarmos
# de 3 ao numero sugerido pela power analysis + 2
n_samples <- 3:(ceiling(pa$n)+2)

# repetindo o experimento 100 vezes
iter.tests <- 1:100 %>% 
  map_df(function(.i){
    # para cada tamanho de amostra aplicamos t.test 
    n_samples %>% 
      map_df(function(n){
      t.test(sample(popCntrl,n), sample(popTreat,n)) %>% 
        tidy() %>% 
        select(p.value) %>% 
        mutate(sample_size=n) %>% 
        return()
      }) 
  })

# plotando os p.values encontrado para cada tamanho de amostra diferente
iter.tests %>% 
  ggplot(aes(x=as.factor(sample_size), y=p.value)) +
  geom_boxplot() +
  geom_hline(yintercept = 0.05, color="red", linetype="dashed")+
  theme_minimal() + 
  labs(title = "Sample size effect in P-Value",
       subtitle = "P-Value distribution in 100 t.test at different samples sizes",
       y = "p.value distribution",
       x = "sample size")


```

Dá para perceber que o teste de hipóteses vindo de populações diferentes passa a rejeitar a hipotese nula, com 0.05 de signficancia, quando o número se aproxima do tamanho da amostra sugerida pela análise de potência. Inclusive dá pra saber com que frequência isso acontece.

```{r}

# encontra qual a propoção de null hypothesis rejected em cada tamanho de amostra
iter.tests %>% 
  mutate( rejected = p.value <=0.05 ) %>% 
  count(rejected, sample_size) %>% 
  mutate(n=n/100) %>% # pct
  filter(rejected==T) %>% # visualizando a porporcao por tamanho de amostra
  ggplot(aes(x=sample_size, y=n)) +
  # linha onde está parametro "power"
  geom_hline(yintercept = pa$power, color="red", linetype="dashed" ) +
  # quantidade de amostras sugerida para encontrar o effect size
  geom_vline(xintercept = ceiling(pa$n), color="red", linetype="dashed") +
  geom_point() + 
  ylim(0,1) +
  theme_minimal() +
  labs(title = "Power",
       subtitle="Probability that your test will find a true statistically significant",
       x="sample size", y="power")
```

Da para ver que a quantidade de vezes que a gente consegue obter significancia 0.05 ao tentar rejeitar a hipótese nula ultrapassa os 80% (power parameter) quando o tamanho da amostra sugerida chega perto do número estimado pela análise de potência, como é de se esperar.

#### What effect size we can detect in a situation?

O outro jeito de usar a análise de potência é para descobrir, dado um determinado cenário de analise, qual o menor tamanho de efeito que nós conseguiríamos comprovar estatísticamente. Por exemplo, imaginem que no cenário acima, tempo de recuperação da COVID-19 ($ \mu=21.9,  \sigma=5.33 $ ) se pesquisadores tivessem feito um trial com 50 pessoas, sendo 25 delas crupo de controle e 25 sob tratamento, qual é o menor efeito do tratamento que poderíamos detectar com significância estatística?

Esse uso é mais direto da fórmula:

```{r}

# Power Analysis 
pa2 <- pwr.2p.test(sig.level = 0.05, power = .8, n=25)
pa2


```

Assim o tamanho do efeito que poderíamos comprovar estatisticamente é `r round(pa$h,3)`, e que neste caso seria de `r round(pa$h * s0,2)` dias ($ h*\sigma $).

### To be continued

In the next post, we'll explorer a use case for power analysis taken from Matthew J. Salganik's book [Bit by Bit: Social Research in the Digital Age](https://www.amazon.com/Bit-Social-Research-Digital-Age/dp/0691158649) shows how difficult it is to measure the return on investment of online ads, even with digital experiments involving millions of customers.

### References

[^ucla]: [Introduction to Power Analysis, UCLA](https://stats.idre.ucla.edu/other/mult-pkg/seminars/intro-power/)

[^covid]: [Estimation of COVID-19 recovery and decease periods in Canada using machine learning algorithms](https://www.medrxiv.org/content/10.1101/2021.07.16.21260675v1.full)

[^stats]: [Power Analysis Overview](https://www.statmethods.net/stats/power.html)

<!-- dolar &#36; --> 