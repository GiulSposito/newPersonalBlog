---
title: Power Analysis | Measuring the ROI in onlines ad campaigns
author: Giuliano Sposito
date: '2021-11-19'
slug: 'bit-by-Bit-power-analysis'
categories:
  - data science
tags:
  - rstats
  - power analysis
  - hypothesis test
  - simulation
  - sample size
subtitle: 'Case Simulation'
lastmod: '2021-11-15T17:38:47-03:00'
draft: yes
authorLink: ''
description: ''
hiddenFromHomePage: no
hiddenFromSearch: no
featuredImage: 'images/guide_to_facebook-ads01.png'
featuredImagePreview: 'images/facebook-ads-scaled.png'
toc:
  enable: yes
math:
  enable: yes
lightgallery: no
license: ''
---

This post explore the **power analysis** technique using case scenarios in the exercises 21 and 24 of Matthew J. Salganik's book [Bit by Bit: Social Research in the Digital Age](https://www.amazon.com/Bit-Social-Research-Digital-Age/dp/0691158649), from chapter 4. 


<!--more-->

### Introduction

Power is the probability of detecting an effect, given that the effect is really there. In other words, it is the probability of rejecting the null hypothesis when it is in fact false. For example, let’s say that we have a simple study with drug A and a placebo group, and that the drug truly is effective; the power is the probability of finding a difference between the two groups.[^ucla]

So, imagine that we had a power of .8 and that this simple study was conducted many times. Having **power of .8 means that 80% of the time**, we would **get a statistically significant difference** between the drug A and placebo groups. This also means that 20% of the times that we run this experiment, we will not obtain a statistically significant effect between the two groups, even though there really is an effect in reality.

Perhaps **the most common use is to determine the necessary number of subjects needed to detect an effect of a given size**. Note that trying to find the absolute, bare minimum number of subjects needed in the study is often not a good idea. Additionally, **power analysis can be used to determine power, given an effect size and the number of subjects available**. You might do this when you know, for example, that only 75 subjects are available (or that you only have the budget for 75 subjects), and you want to know if you will have enough power to justify actually doing the study. In most cases, **there is really no point to conducting a study that is seriously underpowered**. 

Besides the issue of the number of necessary subjects, there are other good reasons for doing a power analysis. For example, a power analysis is often required as part of a grant proposal.  And finally, doing a power analysis is often just part of doing good research; A power analysis is a good way of making sure that you have thought through every aspect of the study and the statistical analysis before you start collecting data.

```{r setup, echo=FALSE}
# default chunk behavior
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache=TRUE)
set.seed(1234) 
```


### Examples

#### Finding The Sample Size

We'll apply *power analysis* in its case more common, to determine the necessary number of subjects to detect a given effect, in this case let's use the [`{pwr package}`](https://cran.r-project.org/web/packages/pwr/pwr.pdf) in a scenario of drug treatment. Lets consider a control group and a treatment group to COVID-19, for example. To simplify the case we assume that the recovery time for COVID-19 is normally distributed around 21.91 days (mean) and standard deviation of 5.33 days[^covid], how many subjects we will have to had to detect a treatment that can shorter the recovery in 5 days?


```{r simpleCasePops}
# install.packages("pwr")
library(pwr)
library(broom)
library(tidyverse)

# covid recovery time (mean and sd)
mo <- 21.91
s0 <- 5.33

# we want to detect 5 day early-recovery time (same standard deviation) 
mt <- mo-5

# simulation the populations
cntrl <- rnorm(10000, mo, s0) #control
treat <- rnorm(10000, mt, s0) #under treatment

# lets see the populations 
data.frame(
  recoveryTime = c(cntrl, treat),
  group = rep(c("control","treat"), each=10000)
) %>%
  ggplot(aes(x=recoveryTime, fill=group, group=group))+
  geom_histogram(alpha=.5, position = 'identity') +
  theme_minimal() +
  labs(title = "Population Distribuition", subtitle = "Comparing control group and treatment group")
```

Geramos duas populações em que a separação da média, ou seja, o tamanho do efeito do tratamento é bem evidente. Vamos fazer a análise de poder para tentar detectar uma separação como essa, de ~5 dias.

A fórmula de calculo do tamanho do efeito para ser usada neste caso é a distância entre médias em desvio padrões[^stats]:

$$ d=\frac{|\mu_{control}-\mu_{treat}|}{\sigma} $$

where $ \mu $ são as médias dos grupos e $ \sigma $ é o desvio padrão, que vamos considerar (para facilitar) o mesmo entre as populações. Assim podemos usar o pacote `{pwr}` para calcular o número de amostras das populações que nos garatam testar, com 80% de segurança, a nulabilidade de uma hipótese com 0.05 de significância.

```{r sampleSize}
# size efect 21.91 to 16.91
# in this case (simple t.test) the effect size is the mean difference in
# standard deviations (like z-score)
es <- (mo-mt)/s0

# Power Analysis 
pa <- pwr.2p.test(sig.level = 0.05, power = .8, h = es)
pa
```

Então, o tamanho mínimo das amostras para esse caso é `r ceiling(pa$n)` indicado pelo parâmetro `n`. Vamos testar:

```{r hypotesesTest}
smpC <- sample(cntrl, ceiling(pa$n))
smpT <- sample(treat, ceiling(pa$n))

t <- t.test(smpT, smpC)
t
```
Podemos ver que conseguimos testar a hipotese com um `p.value` de `r t$p.value`, mostrando que as amostras podem ter vindo de fato de duas populações diferentes.

Para entender melhor o numero, vamos ver como o `p.value` deste caso se comporta diante de tamanho de amostras diferentes (algo como fazer o _p.hacking_):

```{r phaking}

# tamanhos de amostras para testarmos
# de 3 ao numero sugerido pela power analysis + 2
n_samples <- 3:(ceiling(pa$n)+2)

# repetindo o experimento 100 vezes para cada tipo de tamanho de mostras
iter.tests <- 1:100 %>% 
  purrr::map_df(function(.i){
    n_samples %>% 
      purrr::map_df(function(n){
      smpC <- sample(cntrl,n)
      smpT <- sample(treat,n)
      t.test(smpC, smpT) %>% 
        tidy() %>% 
        dplyr::select(p.value) %>% 
        dplyr::mutate(sample_size=n) %>% 
        return()
      }) 
  })

iter.tests %>% 
  ggplot(aes(x=as.factor(sample_size), y=p.value)) +
  geom_boxplot() +
  geom_hline(yintercept = 0.05, color="red", linetype="dashed")+
  theme_minimal()


```

Dá para perceber que o teste de hipóteses vindo de populações diferentes passa a rejeitar a hipotese nula, com 0.05 de signficancia, quando o número se aproxima do tamanho da amostra sugerida pela análise de potência. Inclusive dá pra saber com que frequência isso acontece.


```{r}
iter.tests %>% 
  mutate( rejected = p.value <=0.05 ) %>% 
  count(rejected, sample_size) %>% 
  mutate(n=n/100) %>% 
  filter(rejected==T) %>% 
  ggplot(aes(x=sample_size, y=n)) +
  geom_hline(yintercept = pa$power, color="red", linetype="dashed" ) +
  geom_vline(xintercept = ceiling(pa$n), color="red", linetype="dashed") +
  geom_point() + 
  ylim(0,1) +
  theme_minimal()
```

Da para ver que a quantidade de vezes que a gente consegue obter significancia 0.05 ao tentar rejeitar a hipótese nula ultrapassa os 80% (power parameter) quando o tamanho da amostra sugerida chega perto do número estimado pela análise de potência, como é de se esperar.

#### What effect seze we can detect in a situation?

O outro jeito de usar a análise de potência é para descobrir, dado um determinado cenário de analise, qual o menor tamanho de efeito que nós conseguiríamos comprovar estatísticamente. Por exemplo, imaginem que no cenário acima, tempo de recuperação da COVID-19 ($ \mu=21.9,  \sigma=5.33 $ ) se pesquisadores tivessem feito um trial com 50 pessoas, sendo 25 delas crupo de controle e 25 sob tratamento, qual é o menor efeito do tratamento que poderíamos detectar com significância estatística?

Esse uso é mais direto da fórmula:

```{r}

# Power Analysis 
pa <- pwr.2p.test(sig.level = 0.05, power = .8, n=25)
pa


```
Assim o tamanho do efeito que poderíamos comprovar estatisticamente é `r round(pa$h,3)`, e que neste caso seria de `r round(pa$h * s0,2)` dias ($ h*\sigma $).

```{r rlnorm}

m <- 10
s <- 5
location <- log(m^2 / sqrt(s^2 + m^2))
shape <- sqrt(log(1 + (s^2 / m^2)))
print(paste("location:", location))
print(paste("shape:", shape))
draws3 <- rlnorm(n=1000000, location, shape)
mean(draws3)
sd(draws3)



```


21

Imagine that you are working as a data scientist at a tech company. Someone from the marketing department asks for your help in evaluating an experiment that they are planning in order to measure the return on investment (ROI) for a new online ad campaign. ROI is defined as the net profit from the campaign divided by the cost of the campaign. For example, a campaign that had no effect on sales would have an ROI of −100%; a campaign where profits generated were equal to costs would have an ROI of 0; and a campaign where profits generated were double the cost would have an ROI of 200%.

Before launching the experiment, the marketing department provides you with the following information based on their earlier research (in fact, these values are typical of the real online ad campaigns reported in Lewis and Rao (2015)[^1]):

* The mean sales per customer follows a log-normal distribution with a mean of `$7` and a standard deviation of `$75`.
* The campaign is expected to increase sales by `$0.35` per customer, which corresponds to an increase in profit of `$0.175` per customer.
* The planned size of the experiment is 200,000 people: half in the treatment group and half in the control group.
* The cost of the campaign is `$0.14` per participant.
* The expected ROI for the campaign is 25% [(0.175 − 0.14)/0.14]. In other words, the marketing department believes that for each 100 dollars spent on marketing, the company will earn an additional `$25` in profit.

Write a memo evaluating this proposed experiment. Your memo should use evidence from a simulation that you create, and it should address two major issues: 

1. Would you recommend launching this experiment as planned? If so, why? If not, why not? Be sure to be clear about the criteria that you are using to make this decision. 
1. What sample size would you recommend for this experiment? Again please be sure to be clear about the criteria that you are using to make this decision.

A good memo will address this specific case; a better memo will generalize from this case in one way (e.g., show how the decision changes as a function of the size of the effect of the campaign); and a great memo will present a fully generalized result. Your memo should use graphs to help illustrate
your results.

Here are two hints. First, the marketing department might have provided you with some unnecessary information, and they might have failed to provide you with some necessary information. Second, if you are using R, be aware that [the rlnorm() function does not work the way that many people expect](https://msalganik.wordpress.com/2017/01/21/making-sense-of-the-rlnorm-function-in-r/).

This activity will give you practice with power analysis, creating simulations, and communicating your results with words and graphs. It should help you conduct power analysis for any kind of experiment, not just experiments designed to estimate ROI. This activity assumes that you have some experience
with statistical testing and power analysis. If you are not familiar with power analysis, I recommend that you read “A Power Primer” by Cohen (1992)[^2].

This activity was inspired by a lovely paper by Lewis and Rao (2015)[^1], which vividly illustrates a fundamental statistical limitation of even massive experiments. Their paper—which originally had the provocative title “On the Near-Impossibility of Measuring the Returns to Advertising”—shows how
difficult it is to measure the return on investment of online ads, even with digital experiments involving millions of customers. More generally, Lewis and Rao (2015) illustrate a fundamental statistical fact that is particularly important for digital-age experiments: it is hard to estimate small treatment effects amidst noisy outcome data.

24

Imagine that you have written the memo described above, and someone from the marketing department provides one piece of new information: they expect a 0.4 correlation between sales before and after the experiment. How does this change the recommendations in your memo? (Hint: see section 4.6.2 for more on the difference-of-means estimator and the difference-in-differences estimator.)


### References

[^1]: Lewis, Randall A. and Rao, Justin M., The Unfavorable Economics of Measuring the Returns to Advertising (September 18, 2014). Available at SSRN: https://ssrn.com/abstract=2367103 or http://dx.doi.org/10.2139/ssrn.2367103

[^2]: Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155–159. https://doi.org/10.1037/0033-2909.112.1.155

[^ucla]: [Introduction to Power Analysis, UCLA](https://stats.idre.ucla.edu/other/mult-pkg/seminars/intro-power/)

[^covid]: [Estimation of COVID-19 recovery and decease periods in Canada using machine learning algorithms](https://www.medrxiv.org/content/10.1101/2021.07.16.21260675v1.full)

[^stats]: [Power Analysis Overview](https://www.statmethods.net/stats/power.html)

<!-- dolar &#36; --> 